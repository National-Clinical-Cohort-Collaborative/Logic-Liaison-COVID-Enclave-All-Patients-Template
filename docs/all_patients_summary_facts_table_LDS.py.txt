#* all_patients_summary_facts_table_LDS:
#*   desc: creates person level fact table
#*   ext: py
#*   inputs:
#*   - all_patients_fact_day_table_LDS
#*   - everyone_cohort
#*   - everyone_patient_deaths
#*   - customize_concept_sets
#*   - Sdoh_variables_all_patients
#* 

#Purpose - The purpose of this pipeline is to produce a day level and a persons level fact table for all patients in the N3C enclave.
#Creator/Owner/contact - Andrea Zhou
#Last Update - 11/26/24
#Description - The final step is to aggregate information to create a data frame that contains a single row of data for each patient in the cohort.  This node aggregates all information from the cohort_all_facts_table and summarizes each patient's facts in a single row.

def all_patients_summary_facts_table_LDS(all_patients_fact_day_table_LDS, everyone_cohort, everyone_patient_deaths, Sdoh_variables_all_patients, customize_concept_sets):

    SDOH_vars = Sdoh_variables_all_patients #dataset not joined in, just referenced here so that it will import with template for easy access if user wants to join to their summary tables
    
    deaths_df = everyone_patient_deaths.select('person_id','patient_death')
    df = all_patients_fact_day_table_LDS.drop('patient_death_at_visit', 'during_macrovisit_hospitalization', 'macrovisit_start_date', 'macrovisit_end_date')
    fusion_sheet = customize_concept_sets
  
    df = df.groupby('person_id').agg(
        F.max('BMI_rounded').alias('BMI_max_observed_or_calculated'),
        *[F.max(col).alias(col + '_indicator') for col in df.columns if col not in ('person_id', 'BMI_rounded', 'date', 'had_vaccine_administered')],
        F.sum('had_vaccine_administered').alias('total_number_of_COVID_vaccine_doses'))

    #columns to indicate whether a patient belongs in confirmed or possible subcohorts
    df = df.withColumn('confirmed_covid_patient', 
        F.when((F.col('LL_COVID_diagnosis_indicator') == 1) | (F.col('PCR_AG_Pos_indicator') == 1), 1).otherwise(0))

    df = df.withColumn('possible_covid_patient', 
        F.when(F.col('confirmed_covid_patient') == 1, 0)
        .when(F.col('Antibody_Pos_indicator') == 1, 1)
        .when(F.col('LL_Long_COVID_diagnosis_indicator') == 1, 1)
        .when(F.col('LL_Long_COVID_clinic_visit_indicator') == 1, 1)
        .when(F.col('LL_PNEUMONIADUETOCOVID_indicator') == 1, 1)
        .when(F.col('LL_MISC_indicator') == 1, 1)
        .when(F.col('LL_SUSPECTEDCOVID19_indicator') == 1, 1)
        .otherwise(0))      
    
    #join above tables on patient ID  
    df = df.join(deaths_df, 'person_id', 'left').withColumnRenamed('patient_death', 'patient_death_indicator')
    df = everyone_cohort.join(df, 'person_id','left')
    
    #final fill of null in non-continuous variables with 0
    df = df.na.fill(value=0, subset = [col.name for col in df.schema.fields if not isinstance(col.dataType, DoubleType) | isinstance(col.dataType, LongType)])
    
    return df
        
#################################################
## Global imports and functions included below ##
#################################################

from pyspark.sql import functions as F
from pyspark.sql.types import IntegerType, LongType, DoubleType
